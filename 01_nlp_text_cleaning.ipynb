{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "iDvoYMi95ENa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQK2WV0l4-dI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning HTML"
      ],
      "metadata": {
        "id": "bGc68X9w5WxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(html):\n",
        "\n",
        "    # parse html content\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    for data in soup(['style', 'script', 'code', 'a']):\n",
        "        # Remove tags\n",
        "        data.decompose()\n",
        "\n",
        "    # return data by retrieving the tag content\n",
        "    return ' '.join(soup.stripped_strings)"
      ],
      "metadata": {
        "id": "ckETubYd5HSQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"\"\"<p>My favorite color is <del>blue</del> <ins>red</ins>.</p>\"\"\"\n",
        "clean_html(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SOiidifyK2nJ",
        "outputId": "043d410e-558f-4c30-d952-ac7fb675439e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My favorite color is blue red .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Make the text lowercase. As you probably know, NLP is case-sensitive.\n",
        "2.   Remove line breaks. Again, depending on your source, you might have encoded line breaks.\n",
        "3.   Remove punctuation. This is using the string library. Other punctuation can be added as needed.\n",
        "4.   Remove stop words using the NLTK library. There is a list in the next line to add additional stop words to the function as needed. These might be noisy domain words or anything else that makes the contextless clear.\n",
        "5.  Removing numbers. Optional depending on your data.\n",
        "6.  Stemming or Lemmatization. This process is an argument in the function. You can choose either one via with Stem or Lem. The default is to use none.\n"
      ],
      "metadata": {
        "id": "Q9W9VVoh50oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H3>punctuation</H3>\n",
        "INPUT:\n",
        "`hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp`\n",
        "<br>\n",
        "OUTPUT:\n",
        "`hey amazon my package never arrived please fix asap`"
      ],
      "metadata": {
        "id": "61PLGNkD9I5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"I had such high hopes for this dress 15 size or (my usual size) to work for me.\"\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "ans = text.translate(str.maketrans(\",\", PUNCT_TO_REMOVE))\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "sC7EA1qpHNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the Repetitions of Punctuations"
      ],
      "metadata": {
        "id": "bGqz6yRLG2Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"I had such... high hopes for this dress!!!!\"\n",
        "ans = re.sub(r'(!)1+', ' ', text1)\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPuDzIVNG3dG",
        "outputId": "af239704-2112-43df-e472-79aea10e7c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I had such... high hopes for this dress!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing Text"
      ],
      "metadata": {
        "id": "8nlr8ARECC1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first FIX THIS ASAP! @AmazonHelp\"\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcRjVikLCFKf",
        "outputId": "87b93713-e1fe-4487-c25b-8be9f7a0e76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first fix this asap! @amazonhelp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Unicode Characters"
      ],
      "metadata": {
        "id": "RPR9nsqBCMX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\"\n",
        "\n",
        "text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCFhhcO6CIVS",
        "outputId": "596d1dd7-a88f-4da7-dadb-0b9c713df969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey amazon  my package never arrived  please fix asap amazonhelp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Extra Space"
      ],
      "metadata": {
        "id": "VPCvxmjKGP7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I had such high hopes for         this dress 15 size     or    (my          usual            size) to work for me.\"\n",
        "ans = \" \".join(text.split())\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkUDvkVoGQUX",
        "outputId": "0f419429-6ac4-4009-9e97-f87807c874a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I had such high hopes for this dress 15 size or (my usual size) to work for me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Numbers"
      ],
      "metadata": {
        "id": "NDHtoBGDFbrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I had such high hopes for this dress 15 size or (my usual size) to work for me.\"\n",
        "ans = ''.join([i for i in text if not i.isdigit()])\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElJNz-d8Fcwm",
        "outputId": "d1e1dcc9-f633-4706-b150-b10feb461540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I had such high hopes for this dress  size or (my usual size) to work for me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stopwords"
      ],
      "metadata": {
        "id": "oHYP82vUCVmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v5Zyz3sCRrP",
        "outputId": "fcace820-0536-4e16-8351-134436c5433e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "text = \"my package from amazon never arrived fix this asap\"\n",
        "text = \" \".join([word for word in text.split() if word not in (stop)])\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVrj_colChi3",
        "outputId": "8416ba58-c71a-4f83-c812-42f566c5f9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package amazon never arrived fix asap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "text = \"I had such high hopes for this dress 1-5 size to work for me.\"\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "ans = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "A_RMMsHWKT83",
        "outputId": "499798e9-c269-48c0-a35c-5f6f5e6eef7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I high hopes dress 1-5 size work me.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**, the simpler of the two, groups words by their root stem. This allows us to recognize that ‚Äòjumping‚Äô ‚Äòjumps‚Äô and ‚Äòjumped‚Äô are all rooted to the same verb (jump) and thus are referring to similar problems.\n",
        "<Br>\n",
        "**Lemmatization**, on the other hand, groups words based on root definition, and allows us to differentiate between present, past, and indefinite.\n",
        "So, ‚Äòjumps‚Äô and ‚Äòjump‚Äô are grouped into the present ‚Äòjump‚Äô, as different from all uses of ‚Äòjumped‚Äô which are grouped together as past tense, and all instances of ‚Äòjumping‚Äô which are grouped together as the indefinite (meaning continuing/continuous).\n",
        "\n"
      ],
      "metadata": {
        "id": "OM_x8vwdDMVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "J0phA32GDYwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "words = [\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
        "stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word + \" = \" + stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ompECvgxDKZP",
        "outputId": "25a9b7a9-3a8d-419b-ab79-4a3ba9863dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jump = jump\n",
            "jumped = jump\n",
            "jumps = jump\n",
            "jumping = jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatazing"
      ],
      "metadata": {
        "id": "KCePWmULDglf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccFN_IoJDscn",
        "outputId": "50650475-44da-4ccd-c363-9040c77e537c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "words = [\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in words:\n",
        "  print(word + \" = \" + lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX67j7MPDjC_",
        "outputId": "c1ff1252-853a-48f2-b109-a5d04ce615ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jump = jump\n",
            "jumped = jumped\n",
            "jumps = jump\n",
            "jumping = jumping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Emojis"
      ],
      "metadata": {
        "id": "jtIbvDk-HUwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"U0001F600-U0001F64F\"  # emoticons\n",
        "                           u\"U0001F300-U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"U0001F680-U0001F6FF\"  # transport & map symbols\n",
        "                           u\"U0001F1E0-U0001F1FF\"  # flags (iOS)\n",
        "                           u\"U00002702-U000027B0\"\n",
        "                           u\"U000024C2-U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "remove_emoji(\"game is on üî•üî•\")"
      ],
      "metadata": {
        "id": "dGiHr_tnHd4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Emoticons\n"
      ],
      "metadata": {
        "id": "cK3TqFbHHjjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMOTICONS = {\n",
        "u‚Äù:‚Äë)‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
        "u‚Äù:)‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
        "u‚Äù:-]‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
        "u‚Äù:]‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
        "u‚Äù:-3‚Ä≥:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:3‚Ä≥:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:->‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:>‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù8-)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:o)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:-}‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:}‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:-)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:c)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù:^)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
        "u‚Äù=]‚Äù:‚ÄùHappy face smiley‚Äù\n",
        "}\n",
        "text = \"‚Äò\"I had such high hopes for this dress 15 size really wanted it to work for me :-)‚Äô\n",
        "ans = re.compile(u'(‚Äò + u‚Äô|‚Äô.join(k for k in EMOTICONS) + u‚Äô)‚Äô)\n",
        "ans = ans.sub(r‚Äù,text)\n",
        "-----------\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "    return text\n",
        "text = \"Hello :-)\"\n",
        "convert_emoticons(text)"
      ],
      "metadata": {
        "id": "L-goQSAXHjP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Contractions"
      ],
      "metadata": {
        "id": "Ikmk5keYIp9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJwTBPGIoz_",
        "outputId": "d22fe102-2459-46d9-f2a0-f92102cb96ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "text = \"She'd like to know how I'd do that!\"\n",
        "contractions.fix(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AWM0eFnoItgO",
        "outputId": "5444b7fd-0a6e-4215-aaf7-f0202e9c0bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She would like to know how I would do that!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing HTML Tags"
      ],
      "metadata": {
        "id": "G8rqHzj9JOrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I had such high hopes for this dress 15 size or (my usual size) to work for me.\"\n",
        "\n",
        "without_html = re.sub(pattern=r'', repl='', string=text)\n",
        "print(f\"{without_html}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CnSsHVJPXH",
        "outputId": "9e8bf6c6-e758-4098-e9f2-d5ee30b684ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I had such high hopes for this dress 15 size or (my usual size) to work for me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing & Finding URL"
      ],
      "metadata": {
        "id": "af8b5FkWJ1_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = 'My email is http://abcgmail.com'\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    if token.like_url:\n",
        "        print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvnJ4CiwJ2s_",
        "outputId": "a5290402-0663-4eff-db51-e4d6507aadbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://abcgmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Look at this link http://abcgmail.com for work purpose https://abd.com'\n",
        "text_sp = text.split()\n",
        "ans = ' '.join([i for i in text_sp if 'ht' not in i])\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gFh6OX47J9Xd",
        "outputId": "e38746fe-7cb0-407c-9351-de1308fbe867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Look at this link for work purpose'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing & Finding Email id"
      ],
      "metadata": {
        "id": "qzG07MQHJ6Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = 'My email is abc@gmail.com'\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    if token.like_email:\n",
        "        print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIFkIoCBKBWI",
        "outputId": "409dffa2-e4d9-4532-abc1-0f6d59daf62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'My email is abc@gmail.com for work purpose'\n",
        "text_sp = text.split()\n",
        "ans = ' '.join([i for i in text_sp if '@' not in i])\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "88UezqkBKEqZ",
        "outputId": "41b4f92d-5e0e-4f19-b715-fe15ca2ee68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My email is for work purpose'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = 'My email is abc@gmail.com'\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    if not token.like_email:\n",
        "        print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nwhSiM4KMvo",
        "outputId": "9de8925c-d0d7-4d0b-e203-67d8e3852a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My\n",
            "email\n",
            "is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizing and Spell Check (autocorrect library)"
      ],
      "metadata": {
        "id": "MDkrwhemOYnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ytwsnrOYJE",
        "outputId": "fda31070-5d3a-4c88-bbec-dae967c6eafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=c6fc3094972ed0a017b7586b034ea8093464799406d6b8081e7998acfb88e389\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from autocorrect import Speller\n",
        "text=\"A farmmer will lovdd this food\"\n",
        "#One letter in a word should not be present more than twice in continuation\n",
        "text_correction = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "print(\"Normal Text:n{}\".format(text_correction))\n",
        "spell = Speller(lang='en')\n",
        "ans = spell(text_correction)\n",
        "print(\"After correcting text:n{}\".format(ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iWhArCzOecO",
        "outputId": "6ba7aae0-8263-484a-9f43-1a5c6eca345f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Text:nA farmmer will lovdd this food\n",
            "After correcting text:nA farmer will loved this food\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Words Conversion"
      ],
      "metadata": {
        "id": "TlBaEh2gOkH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\"\"\""
      ],
      "metadata": {
        "id": "LH3K9c6nO5zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)"
      ],
      "metadata": {
        "id": "zO3jXcw3Pz9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text prepration with **Spacy**"
      ],
      "metadata": {
        "id": "YzaFwPf6CmJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def clean_string(text, stem=\"None\"):\n",
        "\n",
        "    final_string = \"\"\n",
        "\n",
        "    # Make lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove line breaks\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    # Remove puncuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Remove stop words\n",
        "    text = text.split()\n",
        "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
        "    useless_words = useless_words + ['hi', 'im']\n",
        "\n",
        "    text_filtered = [word for word in text if not word in useless_words]\n",
        "\n",
        "    # Remove numbers\n",
        "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
        "\n",
        "    # Stem or Lemmatize\n",
        "    if stem == 'Stem':\n",
        "        stemmer = PorterStemmer()\n",
        "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
        "    elif stem == 'Lem':\n",
        "        lem = WordNetLemmatizer()\n",
        "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
        "    elif stem == 'Spacy':\n",
        "        text_filtered = nlp(' '.join(text_filtered))\n",
        "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
        "    else:\n",
        "        text_stemmed = text_filtered\n",
        "\n",
        "    final_string = ' '.join(text_stemmed)\n",
        "\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "-Vz2uZVP5aEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "B7eiql9o6cZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exs = \"\"\"<p><a href=\"https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item\" rel=\"nofollow noreferrer\">https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item</a></p>\\n\\n<p>I have followed the tutorial and have successfully obtained the contents of the file, but where is the file being downloaded. In addition, how do I specify the location of where I want to download the file?</p>\\n\\n<p>Result on Postman\\n<a href=\"https://i.stack.imgur.com/VrdqP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/VrdqP.png\" alt=\"enter image description here\"></a></p>\\n\"\"\""
      ],
      "metadata": {
        "id": "MgjrJMQj6dJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = clean_html(exs)\n",
        "clean_string(x, stem='Stem')"
      ],
      "metadata": {
        "id": "ucXJd5LQ6fmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Resource:<br>\n",
        "https://www.dataknowsall.com/textcleaning.html<br>\n",
        "https://monkeylearn.com/blog/text-cleaning/<br>\n",
        "https://www.analyticsvidhya.com/blog/2022/01/text-cleaning-methods-in-nlp/<br>\n",
        "https://www.analyticsvidhya.com/blog/2022/02/text-cleaning-methods-in-nlp-part-2/"
      ],
      "metadata": {
        "id": "9LQxbIO-7whx"
      }
    }
  ]
}